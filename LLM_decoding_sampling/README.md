# Language Model Decoding

## Overview

Given an input text passage as context, open-ended generation involves generating text that forms a coherent continuation from the given context. This process is fundamental to how language models operate and generate responses.

## Formal Definition

More formally, given a sequence of `m` tokens `x₁ ... xₘ` as context, the task is to generate the next `n` continuation tokens to obtain the completed sequence `x₁ ... xₘ₊ₙ`.

## Probability Model

Models compute `P(x₁:ₘ₊ₙ)` using the common left-to-right decomposition of the text probability:


P(x₁:ₘ₊ₙ) = ∏ᵢ₌₁ᵐ⁺ⁿ P(xᵢ|x₁ ... xᵢ₋₁)

## Paper


#### THE CURIOUS CASE OF NEURAL TEXT DeGENERATION
[paper] (https://arxiv.org/pdf/1904.09751)

### Main Idea

They propose
show that the “unreliable tail” is to blame. This unreliable tail is composed of tens of thousands of
candidate tokens with relatively low probability that are over-represented in the aggregate. he key intuition of Nucleus
Sampling is that the vast majority of probability mass at each time step is concentrated in the nucleus,
a small subset of the vocabulary that tends to range between one and a thousand candidates. Instead
of relying on a fixed top-k, or using a temperature parameter to control the shape of the distribution
without sufficiently suppressing the unreliable tail, we propose sampling from the top-p portion of
the probability mass, expanding and contracting the candidate pool dynamically.

### Maximization-based decoding.
The most commonly used decoding objective, in particular for
directed generation, is maximization-based decoding. Assuming that the model assigns higher probability to higher quality text, these decoding strategies search for the continuation with the highest likelihood. Since finding the optimum argmax sequence from recurrent neural language models or
Transformers is not tractable , common practice is to use beam search  However, several recent studies on open-ended
generation have reported that maximization-based decoding does not lead to high quality text 

### 1. Nucleus Sampling (Top-p)
- Dynamically selects smallest set of tokens whose cumulative probability exceeds threshold p
- Adapts to model's confidence level
- Distribution is rescaled within selected tokens
- Typically uses p values between 0.9-0.99

### 2. Top-k Sampling
- Selects fixed k highest probability tokens
- Samples from these k tokens according to relative probabilities
- Limitations:
  - Fixed k is sub-optimal across contexts
  - Too small k: risks generic text
  - Too large k: may include inappropriate candidates
- Less adaptive than Nucleus Sampling

### 3. Temperature Sampling
- Reshapes probability distribution using temperature parameter t


p(x = Vₗ | x₁:ᵢ₋₁) = exp(uₗ/t) / Σ exp(u'ₗ/t)

- Characteristics:
- t < 1: Favors high probability events
- Lower t: Improves quality but reduces diversity
- Often combined with top-k sampling

## Comparison
| Method | Adaptivity | Control Parameter | Trade-offs |
|--------|------------|-------------------|------------|
| Nucleus | Dynamic | p threshold | Best balance of quality and diversity |
| Top-k | Fixed | k value | Simple but less context-aware |
| Temperature | Continuous | t value | Quality vs. diversity trade-off |

## Best Practices
1. Nucleus Sampling: Preferred for general use
2. Top-k: When computational efficiency is crucial
3. Temperature: Often used in combination with other methods

#### Turning Up the Heat: Min-P Sampling for Creative and Coherent LLM Outputs
This paper explores the Min-P sampling technique, which aims to improve the quality of outputs generated by large language models (LLMs). By adjusting the sampling parameters, Min-P sampling strikes a balance between creativity and coherence, leading to more engaging and contextually appropriate results.

[Read the paper](https://openreview.net/pdf?id=FBkpCyujtS)

